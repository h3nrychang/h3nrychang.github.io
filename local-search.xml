<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1+Ollama+AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！</title>
    <link href="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/"/>
    <url>/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>DeepSeek</code>最近的爆火，离不开他的功能强大以及开源属性，现在关于他的新闻和宣传也是铺天盖地！<br>他在中文、科学计算、编码能力等多项技能都达到甚至超越<strong>行业领先水平</strong>，是妥妥的世界顶尖AI大模型。<br><img src="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg" alt="大模型Benchmark成绩" title="大模型Benchmark成绩"><br>不过，由于DeepSeek网站本地算力不足，使用人数火热且部分算力用于训练下一代大模型，所以使用网页版很容易出现这种现象：问大模型一个问题，等了很久，最终出现了服务器繁忙，未响应的结果。<br><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/123.png" alt="图片"><br>由此网上也有一些DeepSeek被谁攻击了等等的传言，不过这个问题真假性我们暂且不谈，我们主要需要解决DeepSeek使用不稳定，的问题。当然，用大模型来辅助办公，如果网页版上传一些<strong>涉及到保密安全等级</strong>的资料，资料<strong>泄密的风险非常大</strong>。所以我们不如另辟蹊径，搭建一个本地离线版DeepSeek，彻底解决上面的问题。</p><h1 id="本地部署要求"><a href="#本地部署要求" class="headerlink" title="本地部署要求"></a>本地部署要求</h1><h2 id="Deepseek配置要求一览表"><a href="#Deepseek配置要求一览表" class="headerlink" title="Deepseek配置要求一览表"></a>Deepseek配置要求一览表</h2><table><thead><tr><th align="center">模型大小</th><th align="center">最低显存大小</th><th align="center">推荐GPU型号</th><th align="center">RAM要求</th><th align="center">适合场景</th></tr></thead><tbody><tr><td align="center">1.5B</td><td align="center">0.5-3.5G</td><td align="center">核显即可</td><td align="center">4G</td><td align="center">轻量模型，适配市面上99%以上PC设备</td></tr><tr><td align="center">7B</td><td align="center">3.5-14G</td><td align="center">RTX2060</td><td align="center">8G</td><td align="center">通用推理，性能较好且硬件需求适中</td></tr><tr><td align="center">8B</td><td align="center">4-16G</td><td align="center">RTX3060</td><td align="center">16G</td><td align="center">通用推理，性能略高于7B，适合需要更高精度的场景</td></tr><tr><td align="center">14B</td><td align="center">7-28G</td><td align="center">RTX4080</td><td align="center">32G</td><td align="center">高性能推理，适合复杂任务（如数学推理、代码生成）</td></tr><tr><td align="center">32B</td><td align="center">16-64G</td><td align="center">A100 40GB x2</td><td align="center">64G</td><td align="center">专业级推理，适合研究和高精度任务</td></tr><tr><td align="center">70B</td><td align="center">35-140G</td><td align="center">A100 80GB x4</td><td align="center">256G</td><td align="center">大规模推理，适合大规模计算和高复杂度任务</td></tr><tr><td align="center">671B</td><td align="center">335-1340G</td><td align="center">H100集群</td><td align="center">512G</td><td align="center">企业级云计算，需专业级硬件支持</td></tr></tbody></table><p>对于不同的量化版本，显存占用方面，4bit量化版本为原版的四分之一，8bit量化版本为原版的一半。而精度保留率随着参数的提高影响逐渐增大。其中8bit量化版本在32B参数下可以保留90%左右的精度，4bit量化版本仅能保留75%左右，文本输出的不确定性增加，稳定性和质量会大幅下降。<br>所以从输出的质量、稳定性和速度综合来看，我们这次教程选择了14B参数，8位量化版本。</p><h2 id="如何选择适合自己的配置"><a href="#如何选择适合自己的配置" class="headerlink" title="如何选择适合自己的配置"></a>如何选择适合自己的配置</h2><h1 id="搭建本地知识库环境"><a href="#搭建本地知识库环境" class="headerlink" title="搭建本地知识库环境"></a>搭建本地知识库环境</h1><h2 id="环境与配置"><a href="#环境与配置" class="headerlink" title="环境与配置"></a>环境与配置</h2><ul><li>OS: Windows 11 22631.2506</li><li>CPU: AMD Ryzen 9 8945HS</li><li>RAM: 32GB</li><li>GPU: NVIDIA GeForce RTX 4060 Laptop</li><li>VRAM: 8GB</li></ul><h2 id="安装Ollama"><a href="#安装Ollama" class="headerlink" title="安装Ollama"></a>安装Ollama</h2><p>Ollama是一款能够在本地Windows、macOS和Linux系统上运行LLM大语言模型的开源工具。<br>项目地址：<a href="https://github.com/ollama/ollama">Ollama Github地址</a><br>下载链接（Windows）：<a href="https://ollama.com/download/OllamaSetup.exe">Windows版下载</a><br>Linux系统运行下面命令安装：<br>国产统信UOS、deepin等也支持，不过部分系统需要开启sudo（开发者）权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl -fsSL https://ollama.com/install.sh | sh<br></code></pre></td></tr></table></figure><p>先别急着运行，还有环境变量没有配置！</p><h3 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h3><p>右键点击桌面<strong>此电脑</strong>，在弹出窗口中点击<strong>高级系统设置</strong>，选择<strong>环境变量</strong>菜单，添加如下两个环境变量：<br><code>OLLAMA_HOST</code>值为<code>0.0.0.0:11434</code><br><code>OLLAMA_MODELS</code>值为<code>X:\xxxxxx（你想存放模型文件的路径）</code><br>其中第一个环境变量代表ollama接受来自所有网络的请求，且在端口11434开放，第二个环境变量代表模型文件会存放在你配置的路径，如果不配置的话默认会存放在C盘的%USERPROFILE%\AppData\Local\Programs\Ollama下，你的C盘很快就会红温了。</p><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/PixPin_2025-03-05_15-26-26.png" class="" title="alt text"><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/PixPin_2025-03-05_15-28-12.png" class="" title="alt text"><h3 id="OLLAMA基础使用"><a href="#OLLAMA基础使用" class="headerlink" title="OLLAMA基础使用"></a>OLLAMA基础使用</h3><h4 id="启动OLLAMA服务"><a href="#启动OLLAMA服务" class="headerlink" title="启动OLLAMA服务"></a>启动OLLAMA服务</h4><p>安装完成后，启动OLLAMA服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama serve<br></code></pre></td></tr></table></figure><h4 id="下载并运行模型"><a href="#下载并运行模型" class="headerlink" title="下载并运行模型"></a>下载并运行模型</h4><p>运行以下命令可以下载名称为<code>deepseek-r1:14b-qwen-distill-q8_0</code>的模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama pull deepseek-r1:14b-qwen-distill-q8_0<br></code></pre></td></tr></table></figure><p>其他的模型可以在ollama官网的<a href="https://ollama.com/search">模型搜索板块</a>查询。</p><p>等待下载完成后可以运行模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama run deepseek-r1:14b-qwen-distill-q8_0<br></code></pre></td></tr></table></figure><h4 id="管理本地模型"><a href="#管理本地模型" class="headerlink" title="管理本地模型"></a>管理本地模型</h4><p>如果希望管理电脑本地的模型，可以通过下面命令查看、删除模型：<br>查看所有模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama list<br></code></pre></td></tr></table></figure><p>删除某个模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama <span class="hljs-built_in">rm</span> xxxxxx<br></code></pre></td></tr></table></figure><h3 id="通过OLLAMA本地部署Deepseek大模型"><a href="#通过OLLAMA本地部署Deepseek大模型" class="headerlink" title="通过OLLAMA本地部署Deepseek大模型"></a>通过OLLAMA本地部署Deepseek大模型</h3><h3 id="OLLAMA高级功能"><a href="#OLLAMA高级功能" class="headerlink" title="OLLAMA高级功能"></a>OLLAMA高级功能</h3><h2 id="安装AnythingLLM"><a href="#安装AnythingLLM" class="headerlink" title="安装AnythingLLM"></a>安装AnythingLLM</h2><h2 id="导入知识库文件"><a href="#导入知识库文件" class="headerlink" title="导入知识库文件"></a>导入知识库文件</h2><h2 id="任务理解"><a href="#任务理解" class="headerlink" title="任务理解"></a>任务理解</h2>]]></content>
    
    
    
    <tags>
      
      <tag>DeepSeek</tag>
      
      <tag>Ollama</tag>
      
      <tag>AnythingLLM</tag>
      
      <tag>本地知识库</tag>
      
      <tag>大模型</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
