<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1+Ollama+AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！</title>
    <link href="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/"/>
    <url>/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>DeepSeek</code>最近的爆火，离不开他的功能强大以及开源属性，现在关于他的新闻和宣传也是铺天盖地！<br>他在中文、科学计算、编码能力等多项技能都达到甚至超越<strong>行业领先水平</strong>，是妥妥的世界顶尖AI大模型。<br><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/benchmark.jpg" alt="大模型Benchmark成绩" title="大模型Benchmark成绩"><br>不过，由于DeepSeek网站本地算力不足，使用人数火热且部分算力用于训练下一代大模型，所以使用网页版很容易出现这种现象：问大模型一个问题，等了很久，最终出现了服务器繁忙，未响应的结果。<br><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/123.png" alt="图片"><br>由此网上也有一些DeepSeek被谁攻击了等等的传言，不过这个问题真假性我们暂且不谈，我们主要需要解决DeepSeek使用不稳定，的问题。当然，用大模型来辅助办公，如果网页版上传一些<strong>涉及到保密安全等级</strong>的资料，资料<strong>泄密的风险非常大</strong>。所以我们不如另辟蹊径，搭建一个本地离线版DeepSeek，彻底解决上面的问题。</p><h1 id="本地部署要求"><a href="#本地部署要求" class="headerlink" title="本地部署要求"></a>本地部署要求</h1><h2 id="Deepseek配置要求一览表"><a href="#Deepseek配置要求一览表" class="headerlink" title="Deepseek配置要求一览表"></a>Deepseek配置要求一览表</h2><table><thead><tr><th align="center">模型大小</th><th align="center">最低显存大小</th><th align="center">推荐GPU型号</th><th align="center">RAM要求</th><th align="center">常见场景</th></tr></thead><tbody><tr><td align="center">1.5B</td><td align="center">0.5-3.5G</td><td align="center">核显即可</td><td align="center">4G</td><td align="center">轻量模型，适配市面上99%以上PC设备</td></tr><tr><td align="center">7B</td><td align="center">3.5-14G</td><td align="center">RTX2060</td><td align="center">8G</td><td align="center">通用推理，性能较好且硬件需求适中</td></tr><tr><td align="center">8B</td><td align="center">4-16G</td><td align="center">RTX3060</td><td align="center">16G</td><td align="center">通用推理，性能略高于7B，适合需要更高精度的场景</td></tr><tr><td align="center">14B</td><td align="center">7-28G</td><td align="center">RTX4080</td><td align="center">32G</td><td align="center">高性能推理，适合复杂任务（如数学推理、代码生成）</td></tr><tr><td align="center">32B</td><td align="center">16-64G</td><td align="center">A100 40GB x2</td><td align="center">64G</td><td align="center">专业级推理，适合研究和高精度任务</td></tr><tr><td align="center">70B</td><td align="center">35-140G</td><td align="center">A100 80GB x4</td><td align="center">256G</td><td align="center">大规模推理，适合大规模计算和高复杂度任务</td></tr><tr><td align="center">671B</td><td align="center">335-1340G</td><td align="center">H100集群</td><td align="center">512G</td><td align="center">企业级云计算，需专业级硬件支持</td></tr></tbody></table><p>对于不同的量化版本，显存占用方面，4bit量化版本为原版的四分之一，8bit量化版本为原版的一半。而精度保留率随着参数的提高影响逐渐增大。其中8bit量化版本在32B参数下可以保留90%左右的精度，4bit量化版本仅能保留75%左右，文本输出的不确定性增加，稳定性和质量会大幅下降。<br>所以从输出的质量、稳定性和速度综合来看，我们这次教程选择了14B参数，8位量化版本。</p><h2 id="如何选择适合自己的配置"><a href="#如何选择适合自己的配置" class="headerlink" title="如何选择适合自己的配置"></a>如何选择适合自己的配置</h2><h1 id="搭建本地知识库环境"><a href="#搭建本地知识库环境" class="headerlink" title="搭建本地知识库环境"></a>搭建本地知识库环境</h1><h2 id="环境与配置"><a href="#环境与配置" class="headerlink" title="环境与配置"></a>环境与配置</h2><ul><li>OS: Windows 11 22631.2506</li><li>CPU: AMD Ryzen 9 8945HS</li><li>RAM: 32GB</li><li>GPU: NVIDIA GeForce RTX 4060 Laptop</li><li>VRAM: 8GB</li></ul><h2 id="安装Ollama"><a href="#安装Ollama" class="headerlink" title="安装Ollama"></a>安装Ollama</h2><p>Ollama是一款能够在本地Windows、macOS和Linux系统上运行LLM大语言模型的开源工具。<br>项目地址：<a href="https://github.com/ollama/ollama">Ollama Github地址</a><br>下载链接（Windows）：<a href="https://ollama.com/download/OllamaSetup.exe">Windows版下载</a><br>Linux系统运行下面命令安装：<br>国产统信UOS、deepin等也支持，不过部分系统需要开启sudo（开发者）权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl -fsSL https://ollama.com/install.sh | sh<br></code></pre></td></tr></table></figure><p>先别急着运行，还有环境变量没有配置！</p><h3 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h3><p>右键点击桌面<strong>此电脑</strong>，在弹出窗口中点击<strong>高级系统设置</strong>，选择<strong>环境变量</strong>菜单，添加如下两个环境变量：<br><code>OLLAMA_HOST</code>值为<code>0.0.0.0:11434</code><br><code>OLLAMA_MODELS</code>值为<code>X:\xxxxxx（你想存放模型文件的路径）</code><br>其中第一个环境变量代表ollama接受来自所有网络的请求，且在端口11434开放，第二个环境变量代表模型文件会存放在你配置的路径，如果不配置的话默认会存放在C盘的%USERPROFILE%\AppData\Local\Programs\Ollama下，你的C盘很快就会红温了。</p><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/PixPin_2025-03-05_15-26-26.png" class="" title="alt text"><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/PixPin_2025-03-05_15-28-12.png" class="" title="alt text"><h3 id="OLLAMA基础使用"><a href="#OLLAMA基础使用" class="headerlink" title="OLLAMA基础使用"></a>OLLAMA基础使用</h3><h4 id="启动OLLAMA服务"><a href="#启动OLLAMA服务" class="headerlink" title="启动OLLAMA服务"></a>启动OLLAMA服务</h4><p>安装完成后，启动OLLAMA服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama serve<br></code></pre></td></tr></table></figure><h4 id="下载并运行模型"><a href="#下载并运行模型" class="headerlink" title="下载并运行模型"></a>下载并运行模型</h4><p>运行以下命令可以下载名称为<code>deepseek-r1:14b-qwen-distill-q8_0</code>的模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama pull deepseek-r1:14b-qwen-distill-q8_0<br></code></pre></td></tr></table></figure><p>其他的模型可以在ollama官网的<a href="https://ollama.com/search">模型搜索板块</a>查询。</p><p>等待下载完成后可以运行模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama run deepseek-r1:14b-qwen-distill-q8_0<br></code></pre></td></tr></table></figure><h4 id="管理本地模型"><a href="#管理本地模型" class="headerlink" title="管理本地模型"></a>管理本地模型</h4><p>如果希望管理电脑本地的模型，可以通过下面命令查看、删除模型：<br>查看所有模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama list<br></code></pre></td></tr></table></figure><p>删除某个模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama <span class="hljs-built_in">rm</span> xxxxxx<br></code></pre></td></tr></table></figure><h3 id="通过OLLAMA本地部署Deepseek大模型"><a href="#通过OLLAMA本地部署Deepseek大模型" class="headerlink" title="通过OLLAMA本地部署Deepseek大模型"></a>通过OLLAMA本地部署Deepseek大模型</h3><p>DeepSeek 开源的大模型，有些小伙伴在本地部署下载 DeepSeek 模型时会看到 Qwen 与 Llama 蒸馏模型，以及 Q2、Q3、Q4、Q5、Q8 等的代号，还有面对1.5~671B的众多型号尺寸的deepseek大模型，我们应该如何选择适合自己电脑的模型呢？如果在常见PC上运行deepseek的话，可以从下面的表格中选择：</p><table><thead><tr><th align="center">模型名称</th><th align="center">参数量</th><th align="center">基础架构</th><th align="center">适合场景</th></tr></thead><tbody><tr><td align="center">DeepSeek-R1-Distill-Qwen-1.5B</td><td align="center">1.5B</td><td align="center">Qwen2.5</td><td align="center">适合移动设备</td></tr><tr><td align="center">DeepSeek-R1-Distill-Qwen-7B</td><td align="center">7B</td><td align="center">Qwen2.5</td><td align="center">适合普通文本生成工具</td></tr><tr><td align="center">DeepSeek-R1-Distill-Llama-8B</td><td align="center">8B</td><td align="center">Llama3.1</td><td align="center">适合小型企业日常文本处理</td></tr><tr><td align="center">DeepSeek-R1-Distill-Qwen-14B</td><td align="center">14B</td><td align="center">Qwen2.5</td><td align="center">适合桌面级应用</td></tr><tr><td align="center">DeepSeek-R1-Distill-Qwen-32B</td><td align="center">32B</td><td align="center">Qwen2.5</td><td align="center">适合专业领域知识问答系统</td></tr><tr><td align="center">DeepSeek-R1-Distill-Llama-70B</td><td align="center">70B</td><td align="center">Llama3.3</td><td align="center">适合科研、学术研究等高要求场景</td></tr></tbody></table><hr><h4 id="Qwen和Llama的区别"><a href="#Qwen和Llama的区别" class="headerlink" title="Qwen和Llama的区别"></a>Qwen和Llama的区别</h4><div class="note note-primary">            <p>Qwen (通义千问)</p>          </div><p><strong>开发者</strong>：阿里巴巴达摩院<br><strong>架构</strong>：基于 Transformer，支持更长上下文窗口<br><strong>训练数据</strong>：侧重中文语料，兼顾多语言<br><strong>应用场景</strong>：中文 NLP 任务优化  </p><div class="note note-info">            <p>Llama (Meta)</p>          </div><p><strong>开发者</strong>：Meta (Facebook)<br><strong>架构</strong>：基于 Transformer，优化稀疏注意力机制<br><strong>训练数据</strong>：以英文为主，涵盖部分多语言数据<br><strong>应用场景</strong>：通用任务，适配英文环境更好  </p><h4 id="量化介绍"><a href="#量化介绍" class="headerlink" title="量化介绍"></a>量化介绍</h4><p>Q2、Q3、Q4、Q5、Q8 的代号属于模型量化技术的标识符，主要取决于量化工具（如 GGUF 格式）。量化旨在降低模型存储和计算成本，常见规则如下：</p><p><strong>Q2_K</strong></p><ul><li>位宽：2-bit</li><li>精度损失：高</li><li>内存占用：极低</li><li>推理速度：极快</li></ul><p><strong>Q3_K_M</strong></p><ul><li>位宽：3-bit</li><li>精度损失：中</li><li>内存占用：低</li><li>推理速度：快</li></ul><p><strong>Q4_K_S</strong></p><ul><li>位宽：4-bit</li><li>精度损失：低</li><li>内存占用：中等</li><li>推理速度：中等</li></ul><p><strong>Q5_K_M</strong></p><ul><li>位宽：5-bit</li><li>精度损失：极低</li><li>内存占用：较高</li><li>推理速度：较慢</li></ul><p><strong>Q8_0</strong></p><ul><li>位宽：8-bit</li><li>精度损失：可忽略</li><li>内存占用：高</li><li>推理速度：慢</li></ul><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>在本地电脑进行部署模型时，如果电脑没有独立显卡，推荐部署1.5B的无量化版本，有独立显卡可以尝试7B的无量化或14B的量化版本</p><h3 id="OLLAMA高级功能"><a href="#OLLAMA高级功能" class="headerlink" title="OLLAMA高级功能"></a>OLLAMA高级功能</h3><h4 id="通用功能"><a href="#通用功能" class="headerlink" title="通用功能"></a>通用功能</h4><p>查看日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama logs<br></code></pre></td></tr></table></figure><p>清理缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama clean<br></code></pre></td></tr></table></figure><p>查看模型详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama show &lt;model-name&gt;<br></code></pre></td></tr></table></figure><p>查看模型配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama config &lt;model-name&gt;<br></code></pre></td></tr></table></figure><p>查看模型性能信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama perf &lt;model-name&gt;<br></code></pre></td></tr></table></figure><h4 id="自定义模型"><a href="#自定义模型" class="headerlink" title="自定义模型"></a>自定义模型</h4><ol><li>创建自定义模型<br>基于现有模型创建自定义模型：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama create &lt;custom-model-name&gt; -f &lt;Modelfile&gt;<br></code></pre></td></tr></table></figure><ol start="2"><li>复制模型<br>复制一个已经存在的模型：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama <span class="hljs-built_in">cp</span> &lt;source-model-name&gt; &lt;new-model-name&gt;<br></code></pre></td></tr></table></figure><ol start="3"><li>推送自定义模型到平台或模型库<br>将自定义模型推送到模型库：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama push &lt;model-name&gt;<br></code></pre></td></tr></table></figure><h2 id="AnythingLLM"><a href="#AnythingLLM" class="headerlink" title="AnythingLLM"></a>AnythingLLM</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>官方网址：<a href="https://anythingllm.com/desktop">点击前往</a></p><h3 id="创建知识库"><a href="#创建知识库" class="headerlink" title="创建知识库"></a>创建知识库</h3><p>先新建一个工作区，然后点击工作区旁边的上传按钮，可以将文件上传到工作区当中，要注意上传之后，最好勾选旁边的置顶选项。<br><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/2189319-20250207165412530-860111683.png" alt="alt text"><br><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/2189319-20250207165523954-1128187426.png" alt="alt text"></p><h3 id="使用知识库内容提问"><a href="#使用知识库内容提问" class="headerlink" title="使用知识库内容提问"></a>使用知识库内容提问</h3><p>使用配置好知识库的Deepseek R1 模型，这里需要在聊天设置中配置相关的聊天提示，模型才能够更好的理解你的任务</p><p><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/2189319-20250207170014598-2108896669.png" alt="alt text"><br><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/2189319-20250207165721919-111817542.png" alt="alt text"></p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepSeek</tag>
      
      <tag>Ollama</tag>
      
      <tag>AnythingLLM</tag>
      
      <tag>本地知识库</tag>
      
      <tag>大模型</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
