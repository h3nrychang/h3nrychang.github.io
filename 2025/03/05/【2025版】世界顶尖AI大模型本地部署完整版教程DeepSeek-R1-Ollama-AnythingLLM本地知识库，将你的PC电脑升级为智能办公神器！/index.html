

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Henry Chang">
  <meta name="keywords" content="">
  
    <meta name="description" content="如何使用DeepSeek-R1+Ollama+AnythingLLM部署属于自己的本地知识库呢？看下面这篇就够了！">
<meta property="og:type" content="article">
<meta property="og:title" content="【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1+Ollama+AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！">
<meta property="og:url" content="http://example.com/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/index.html">
<meta property="og:site_name" content="H3NRY&#39;s Blog">
<meta property="og:description" content="如何使用DeepSeek-R1+Ollama+AnythingLLM部署属于自己的本地知识库呢？看下面这篇就够了！">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/cover/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1+Ollama+AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81.png">
<meta property="article:published_time" content="2025-03-05T02:07:04.000Z">
<meta property="article:modified_time" content="2025-03-07T04:02:00.856Z">
<meta property="article:author" content="Henry Chang">
<meta property="article:tag" content="DeepSeek">
<meta property="article:tag" content="Ollama">
<meta property="article:tag" content="AnythingLLM">
<meta property="article:tag" content="本地知识库">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/cover/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1+Ollama+AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81.png">
  
  
  
  <title>【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1+Ollama+AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！ - H3NRY&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"KLdGgPtMLJqCHfUEcScXUCH6-MdYXbMMI","app_key":"CNJz3FjPdGeZOgB9VtIzAxG2","server_url":"https://kldggptm.api.lncldglobal.com","path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>H3NRY的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1+Ollama+AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-05 10:07" pubdate>
          March 5, 2025 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          1.8k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          16 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1+Ollama+AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>DeepSeek</code>最近的爆火，离不开他的功能强大以及开源属性，现在关于他的新闻和宣传也是铺天盖地！<br>他在中文、科学计算、编码能力等多项技能都达到甚至超越<strong>行业领先水平</strong>，是妥妥的世界顶尖AI大模型。<br><img src="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg" srcset="/img/loading.gif" lazyload alt="大模型Benchmark成绩" title="大模型Benchmark成绩"><br>不过，由于DeepSeek网站本地算力不足，使用人数火热且部分算力用于训练下一代大模型，所以使用网页版很容易出现这种现象：问大模型一个问题，等了很久，最终出现了服务器繁忙，未响应的结果。<br><img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/123.png" srcset="/img/loading.gif" lazyload alt="图片"><br>由此网上也有一些DeepSeek被谁攻击了等等的传言，不过这个问题真假性我们暂且不谈，我们主要需要解决DeepSeek使用不稳定，的问题。当然，用大模型来辅助办公，如果网页版上传一些<strong>涉及到保密安全等级</strong>的资料，资料<strong>泄密的风险非常大</strong>。所以我们不如另辟蹊径，搭建一个本地离线版DeepSeek，彻底解决上面的问题。</p>
<h1 id="本地部署要求"><a href="#本地部署要求" class="headerlink" title="本地部署要求"></a>本地部署要求</h1><h2 id="Deepseek配置要求一览表"><a href="#Deepseek配置要求一览表" class="headerlink" title="Deepseek配置要求一览表"></a>Deepseek配置要求一览表</h2><table>
<thead>
<tr>
<th align="center">模型大小</th>
<th align="center">最低显存大小</th>
<th align="center">推荐GPU型号</th>
<th align="center">RAM要求</th>
<th align="center">常见场景</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1.5B</td>
<td align="center">0.5-3.5G</td>
<td align="center">核显即可</td>
<td align="center">4G</td>
<td align="center">轻量模型，适配市面上99%以上PC设备</td>
</tr>
<tr>
<td align="center">7B</td>
<td align="center">3.5-14G</td>
<td align="center">RTX2060</td>
<td align="center">8G</td>
<td align="center">通用推理，性能较好且硬件需求适中</td>
</tr>
<tr>
<td align="center">8B</td>
<td align="center">4-16G</td>
<td align="center">RTX3060</td>
<td align="center">16G</td>
<td align="center">通用推理，性能略高于7B，适合需要更高精度的场景</td>
</tr>
<tr>
<td align="center">14B</td>
<td align="center">7-28G</td>
<td align="center">RTX4080</td>
<td align="center">32G</td>
<td align="center">高性能推理，适合复杂任务（如数学推理、代码生成）</td>
</tr>
<tr>
<td align="center">32B</td>
<td align="center">16-64G</td>
<td align="center">A100 40GB x2</td>
<td align="center">64G</td>
<td align="center">专业级推理，适合研究和高精度任务</td>
</tr>
<tr>
<td align="center">70B</td>
<td align="center">35-140G</td>
<td align="center">A100 80GB x4</td>
<td align="center">256G</td>
<td align="center">大规模推理，适合大规模计算和高复杂度任务</td>
</tr>
<tr>
<td align="center">671B</td>
<td align="center">335-1340G</td>
<td align="center">H100集群</td>
<td align="center">512G</td>
<td align="center">企业级云计算，需专业级硬件支持</td>
</tr>
</tbody></table>
<p>对于不同的量化版本，显存占用方面，4bit量化版本为原版的四分之一，8bit量化版本为原版的一半。而精度保留率随着参数的提高影响逐渐增大。其中8bit量化版本在32B参数下可以保留90%左右的精度，4bit量化版本仅能保留75%左右，文本输出的不确定性增加，稳定性和质量会大幅下降。<br>所以从输出的质量、稳定性和速度综合来看，我们这次教程选择了14B参数，8位量化版本。</p>
<h2 id="如何选择适合自己的配置"><a href="#如何选择适合自己的配置" class="headerlink" title="如何选择适合自己的配置"></a>如何选择适合自己的配置</h2><h1 id="搭建本地知识库环境"><a href="#搭建本地知识库环境" class="headerlink" title="搭建本地知识库环境"></a>搭建本地知识库环境</h1><h2 id="环境与配置"><a href="#环境与配置" class="headerlink" title="环境与配置"></a>环境与配置</h2><ul>
<li>OS: Windows 11 22631.2506</li>
<li>CPU: AMD Ryzen 9 8945HS</li>
<li>RAM: 32GB</li>
<li>GPU: NVIDIA GeForce RTX 4060 Laptop</li>
<li>VRAM: 8GB</li>
</ul>
<h2 id="安装Ollama"><a href="#安装Ollama" class="headerlink" title="安装Ollama"></a>安装Ollama</h2><p>Ollama是一款能够在本地Windows、macOS和Linux系统上运行LLM大语言模型的开源工具。<br>项目地址：<a target="_blank" rel="noopener" href="https://github.com/ollama/ollama">Ollama Github地址</a><br>下载链接（Windows）：<a target="_blank" rel="noopener" href="https://ollama.com/download/OllamaSetup.exe">Windows版下载</a><br>Linux系统运行下面命令安装：<br>国产统信UOS、deepin等也支持，不过部分系统需要开启sudo（开发者）权限。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl -fsSL https://ollama.com/install.sh | sh<br></code></pre></td></tr></table></figure>
<p>先别急着运行，还有环境变量没有配置！</p>
<h3 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h3><p>右键点击桌面<strong>此电脑</strong>，在弹出窗口中点击<strong>高级系统设置</strong>，选择<strong>环境变量</strong>菜单，添加如下两个环境变量：<br><code>OLLAMA_HOST</code>值为<code>0.0.0.0:11434</code><br><code>OLLAMA_MODELS</code>值为<code>X:\xxxxxx（你想存放模型文件的路径）</code><br>其中第一个环境变量代表ollama接受来自所有网络的请求，且在端口11434开放，第二个环境变量代表模型文件会存放在你配置的路径，如果不配置的话默认会存放在C盘的%USERPROFILE%\AppData\Local\Programs\Ollama下，你的C盘很快就会红温了。</p>
<img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/PixPin_2025-03-05_15-26-26.png" srcset="/img/loading.gif" lazyload class="" title="alt text">
<img src="/2025/03/05/%E3%80%902025%E7%89%88%E3%80%91%E4%B8%96%E7%95%8C%E9%A1%B6%E5%B0%96AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88%E6%95%99%E7%A8%8BDeepSeek-R1-Ollama-AnythingLLM%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84PC%E7%94%B5%E8%84%91%E5%8D%87%E7%BA%A7%E4%B8%BA%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8%EF%BC%81/PixPin_2025-03-05_15-28-12.png" srcset="/img/loading.gif" lazyload class="" title="alt text">

<h3 id="OLLAMA基础使用"><a href="#OLLAMA基础使用" class="headerlink" title="OLLAMA基础使用"></a>OLLAMA基础使用</h3><h4 id="启动OLLAMA服务"><a href="#启动OLLAMA服务" class="headerlink" title="启动OLLAMA服务"></a>启动OLLAMA服务</h4><p>安装完成后，启动OLLAMA服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama serve<br></code></pre></td></tr></table></figure>

<h4 id="下载并运行模型"><a href="#下载并运行模型" class="headerlink" title="下载并运行模型"></a>下载并运行模型</h4><p>运行以下命令可以下载名称为<code>deepseek-r1:14b-qwen-distill-q8_0</code>的模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama pull deepseek-r1:14b-qwen-distill-q8_0<br></code></pre></td></tr></table></figure>
<p>其他的模型可以在ollama官网的<a target="_blank" rel="noopener" href="https://ollama.com/search">模型搜索板块</a>查询。</p>
<p>等待下载完成后可以运行模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama run deepseek-r1:14b-qwen-distill-q8_0<br></code></pre></td></tr></table></figure>

<h4 id="管理本地模型"><a href="#管理本地模型" class="headerlink" title="管理本地模型"></a>管理本地模型</h4><p>如果希望管理电脑本地的模型，可以通过下面命令查看、删除模型：<br>查看所有模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama list<br></code></pre></td></tr></table></figure>
<p>删除某个模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama <span class="hljs-built_in">rm</span> xxxxxx<br></code></pre></td></tr></table></figure>


<h3 id="通过OLLAMA本地部署Deepseek大模型"><a href="#通过OLLAMA本地部署Deepseek大模型" class="headerlink" title="通过OLLAMA本地部署Deepseek大模型"></a>通过OLLAMA本地部署Deepseek大模型</h3><p>DeepSeek 开源的大模型，有些小伙伴在本地部署下载 DeepSeek 模型时会看到 Qwen 与 Llama 蒸馏模型，以及 Q2、Q3、Q4、Q5、Q8 等的代号，还有面对1.5~671B的众多型号尺寸的deepseek大模型，我们应该如何选择适合自己电脑的模型呢？如果在常见PC上运行deepseek的话，可以从下面的表格中选择：</p>
<table>
<thead>
<tr>
<th align="center">模型名称</th>
<th align="center">参数量</th>
<th align="center">基础架构</th>
<th align="center">适合场景</th>
</tr>
</thead>
<tbody><tr>
<td align="center">DeepSeek-R1-Distill-Qwen-1.5B</td>
<td align="center">1.5B</td>
<td align="center">Qwen2.5</td>
<td align="center">适合移动设备</td>
</tr>
<tr>
<td align="center">DeepSeek-R1-Distill-Qwen-7B</td>
<td align="center">7B</td>
<td align="center">Qwen2.5</td>
<td align="center">适合普通文本生成工具</td>
</tr>
<tr>
<td align="center">DeepSeek-R1-Distill-Llama-8B</td>
<td align="center">8B</td>
<td align="center">Llama3.1</td>
<td align="center">适合小型企业日常文本处理</td>
</tr>
<tr>
<td align="center">DeepSeek-R1-Distill-Qwen-14B</td>
<td align="center">14B</td>
<td align="center">Qwen2.5</td>
<td align="center">适合桌面级应用</td>
</tr>
<tr>
<td align="center">DeepSeek-R1-Distill-Qwen-32B</td>
<td align="center">32B</td>
<td align="center">Qwen2.5</td>
<td align="center">适合专业领域知识问答系统</td>
</tr>
<tr>
<td align="center">DeepSeek-R1-Distill-Llama-70B</td>
<td align="center">70B</td>
<td align="center">Llama3.3</td>
<td align="center">适合科研、学术研究等高要求场景</td>
</tr>
</tbody></table>
<hr>
<h4 id="Qwen和Llama的区别"><a href="#Qwen和Llama的区别" class="headerlink" title="Qwen和Llama的区别"></a>Qwen和Llama的区别</h4><div class="note note-primary">
            <p>Qwen (通义千问)</p>
          </div>
<p><strong>开发者</strong>：阿里巴巴达摩院<br><strong>架构</strong>：基于 Transformer，支持更长上下文窗口<br><strong>训练数据</strong>：侧重中文语料，兼顾多语言<br><strong>应用场景</strong>：中文 NLP 任务优化  </p>
<div class="note note-info">
            <p>Llama (Meta)</p>
          </div>
<p><strong>开发者</strong>：Meta (Facebook)<br><strong>架构</strong>：基于 Transformer，优化稀疏注意力机制<br><strong>训练数据</strong>：以英文为主，涵盖部分多语言数据<br><strong>应用场景</strong>：通用任务，适配英文环境更好  </p>
<h4 id="量化介绍"><a href="#量化介绍" class="headerlink" title="量化介绍"></a>量化介绍</h4><p>Q2、Q3、Q4、Q5、Q8 的代号属于模型量化技术的标识符，主要取决于量化工具（如 GGUF 格式）。量化旨在降低模型存储和计算成本，常见规则如下：</p>
<p><strong>Q2_K</strong></p>
<ul>
<li>位宽：2-bit</li>
<li>精度损失：高</li>
<li>内存占用：极低</li>
<li>推理速度：极快</li>
</ul>
<p><strong>Q3_K_M</strong></p>
<ul>
<li>位宽：3-bit</li>
<li>精度损失：中</li>
<li>内存占用：低</li>
<li>推理速度：快</li>
</ul>
<p><strong>Q4_K_S</strong></p>
<ul>
<li>位宽：4-bit</li>
<li>精度损失：低</li>
<li>内存占用：中等</li>
<li>推理速度：中等</li>
</ul>
<p><strong>Q5_K_M</strong></p>
<ul>
<li>位宽：5-bit</li>
<li>精度损失：极低</li>
<li>内存占用：较高</li>
<li>推理速度：较慢</li>
</ul>
<p><strong>Q8_0</strong></p>
<ul>
<li>位宽：8-bit</li>
<li>精度损失：可忽略</li>
<li>内存占用：高</li>
<li>推理速度：慢</li>
</ul>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>在本地电脑进行部署模型时，如果电脑没有独立显卡，推荐部署1.5B的无量化版本，有独立显卡可以尝试7B的无量化或14B的量化版本</p>
<h3 id="OLLAMA高级功能"><a href="#OLLAMA高级功能" class="headerlink" title="OLLAMA高级功能"></a>OLLAMA高级功能</h3><h4 id="通用功能"><a href="#通用功能" class="headerlink" title="通用功能"></a>通用功能</h4><p>查看日志</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama logs<br></code></pre></td></tr></table></figure>
<p>清理缓存</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama clean<br></code></pre></td></tr></table></figure>
<p>查看模型详细信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama show &lt;model-name&gt;<br></code></pre></td></tr></table></figure>
<p>查看模型配置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama config &lt;model-name&gt;<br></code></pre></td></tr></table></figure>
<p>查看模型性能信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama perf &lt;model-name&gt;<br></code></pre></td></tr></table></figure>

<h4 id="自定义模型"><a href="#自定义模型" class="headerlink" title="自定义模型"></a>自定义模型</h4><ol>
<li>创建自定义模型<br>基于现有模型创建自定义模型：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama create &lt;custom-model-name&gt; -f &lt;Modelfile&gt;<br></code></pre></td></tr></table></figure>
<ol start="2">
<li>复制模型<br>复制一个已经存在的模型：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama <span class="hljs-built_in">cp</span> &lt;source-model-name&gt; &lt;new-model-name&gt;<br></code></pre></td></tr></table></figure>
<ol start="3">
<li>推送自定义模型到平台或模型库<br>将自定义模型推送到模型库：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama push &lt;model-name&gt;<br></code></pre></td></tr></table></figure>

<h2 id="安装AnythingLLM"><a href="#安装AnythingLLM" class="headerlink" title="安装AnythingLLM"></a>安装AnythingLLM</h2><p>官方网址：<a target="_blank" rel="noopener" href="https://anythingllm.com/desktop">点击前往</a></p>
<h2 id="创建知识库"><a href="#创建知识库" class="headerlink" title="创建知识库"></a>创建知识库</h2><p>先新建一个工作区，然后点击工作区旁边的上传按钮，可以将文件上传到工作区当中，要注意上传之后，最好勾选旁边的置顶选项。<br><img src="https://img2024.cnblogs.com/blog/2189319/202502/2189319-20250207165412530-860111683.png" srcset="/img/loading.gif" lazyload alt="alt text"><br><img src="https://img2024.cnblogs.com/blog/2189319/202502/2189319-20250207165523954-1128187426.png" srcset="/img/loading.gif" lazyload alt="alt text"></p>
<p>使用配置好知识库的Deepseek R1 模型，这里需要在聊天设置中配置相关的聊天提示，模型才能够更好的理解你的任务</p>
<p><img src="https://img2024.cnblogs.com/blog/2189319/202502/2189319-20250207170014598-2108896669.png" srcset="/img/loading.gif" lazyload alt="alt text"><br><img src="https://img2024.cnblogs.com/blog/2189319/202502/2189319-20250207165721919-111817542.png" srcset="/img/loading.gif" lazyload alt="alt text"></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/DeepSeek/" class="print-no-link">#DeepSeek</a>
      
        <a href="/tags/Ollama/" class="print-no-link">#Ollama</a>
      
        <a href="/tags/AnythingLLM/" class="print-no-link">#AnythingLLM</a>
      
        <a href="/tags/%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93/" class="print-no-link">#本地知识库</a>
      
        <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="print-no-link">#大模型</a>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1+Ollama+AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！</div>
      <div>http://example.com/2025/03/05/【2025版】世界顶尖AI大模型本地部署完整版教程DeepSeek-R1-Ollama-AnythingLLM本地知识库，将你的PC电脑升级为智能办公神器！/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Henry Chang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>March 5, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="/js/leancloud.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
